# Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder (ACL 2025)

Code for "Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder". 

## Evaluation on What'sUp and MMVP/MMVP-VLM

For CLIP-ViT-L/14-336px:

For LLaVA-1.5-7B, Phi-3-V-3.8B, LLaMA-3-V-8B:

## Evaluation on Other Benchmarks

The code is based on 

## Ablation on Training Data

We use the [OpenCLIP](https://github.com/mlfoundations/open_clip) to train the CLIP, SigLIP, and EVA-CLIP model on converted LLaVA-1.5's data.

## Ablation on Token Usage



## Ablation on Language Model



## Ablation on Alignment Architecture and Prompt


